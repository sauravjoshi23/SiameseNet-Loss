{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "siamese-contrastive-loss.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "toc": true,
        "id": "koty8IglfEJi"
      },
      "source": [
        "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
        "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data-Cleaning\" data-toc-modified-id=\"Data-Cleaning-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data Cleaning</a></span></li><li><span><a href=\"#Data-Split\" data-toc-modified-id=\"Data-Split-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data Split</a></span></li></ul></div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxcoNPQ1fEJq",
        "outputId": "b937d5f5-06f9-4490-cbd4-007eddf2b14a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3uvUz3Qfh1i",
        "outputId": "d0c2efd9-7067-4e33-d97a-6a214b954272"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "N3nfdBdofEJr"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import warnings\n",
        "import time\n",
        "import datetime\n",
        "import re\n",
        "import string\n",
        "import itertools\n",
        "import pickle\n",
        "import joblib\n",
        "import nltk\n",
        "import csv\n",
        "\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "stop = set(stopwords.words('english'))\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Input, Concatenate, Conv2D, Flatten, Dense, Embedding, LSTM\n",
        "from keras.models import Model\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "from keras.regularizers import l2\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
        "from keras.models import Model\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.pooling import MaxPooling2D\n",
        "from keras.layers.merge import Concatenate\n",
        "from keras.layers.core import Lambda, Flatten, Dense\n",
        "from keras.initializers import glorot_uniform\n",
        "from keras.layers import Input, Dense, Flatten, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract, Add, Conv2D\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "5BlS2QUJfEJr",
        "outputId": "0cd183ce-811e-4cfb-ea22-bedf24b4fbeb"
      },
      "source": [
        "df = pd.read_csv('tweets.csv', encoding='utf-8', error_bad_lines=False, sep=',')\n",
        "df.drop(['keyword', 'location', 'id'], axis=1, inplace=True)\n",
        "display(df.sample(5))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>482</th>\n",
              "      <td>The guy next to me is texting people “arson fa...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1523</th>\n",
              "      <td>NATO 'Bombed' Libya's Statehood - Russian Fore...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11165</th>\n",
              "      <td>3. Tens of thousands of American soldiers died...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1336</th>\n",
              "      <td>Yo! I was just telling my boyfriend that I was...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6521</th>\n",
              "      <td>#Vulnerability in #Zoom video conference app l...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text  target\n",
              "482    The guy next to me is texting people “arson fa...       0\n",
              "1523   NATO 'Bombed' Libya's Statehood - Russian Fore...       0\n",
              "11165  3. Tens of thousands of American soldiers died...       1\n",
              "1336   Yo! I was just telling my boyfriend that I was...       0\n",
              "6521   #Vulnerability in #Zoom video conference app l...       0"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "immmR1yYfEJs",
        "outputId": "15a320e5-7881-4ee0-a6f5-f69e96a6010a"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11370, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbSWwM-mfEJs",
        "outputId": "bef90317-3a70-4dbb-ae54-57a27892794d"
      },
      "source": [
        "df['target'].value_counts()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    9256\n",
              "1    2114\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwGBCrznfEJs"
      },
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poC0Gk90fEJt"
      },
      "source": [
        "def remove_url(text):\n",
        "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url.sub(r'', text)\n",
        "\n",
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\n",
        "        '['\n",
        "        u'\\U0001F600-\\U0001F64F'  # emoticons\n",
        "        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
        "        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
        "        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n",
        "        u'\\U00002702-\\U000027B0'\n",
        "        u'\\U000024C2-\\U0001F251'\n",
        "        ']+',\n",
        "        flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "def remove_html(text):\n",
        "    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
        "    return re.sub(html, '', text)\n",
        "\n",
        "def remove_punct(text):\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    return text.translate(table)\n",
        "\n",
        "df['clean_text'] = df['text'].apply(lambda x: remove_url(str(x)))\n",
        "df['clean_text'] = df['clean_text'].apply(lambda x: remove_emoji(str(x)))\n",
        "df['clean_text'] = df['clean_text'].apply(lambda x: remove_html(str(x)))\n",
        "df['clean_text'] = df['clean_text'].apply(lambda x: remove_punct(str(x)))\n",
        "df['clean_text'] = df['clean_text'].apply(word_tokenize)\n",
        "df['clean_text'] = df['clean_text'].apply(lambda x: [word.lower() for word in x])\n",
        "df['clean_text'] = df['clean_text'].apply(lambda x: [word for word in x if word not in stop])\n",
        "df['clean_text'] = df['clean_text'].apply(nltk.tag.pos_tag)\n",
        "\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "    \n",
        "df['clean_text'] = df['clean_text'].apply(\n",
        "    lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])\n",
        "wnl = WordNetLemmatizer()\n",
        "df['clean_text'] = df['clean_text'].apply(\n",
        "    lambda x: [wnl.lemmatize(word, tag) for word, tag in x])\n",
        "df['clean_text'] = df['clean_text'].apply(\n",
        "    lambda x: [word for word in x if word not in stop])\n",
        "df['clean_text'] = [' '.join(map(str, l)) for l in df['clean_text']]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "qVQaKcaIfEJt",
        "outputId": "287b15bd-039c-475e-d76e-05f4d01fc60c"
      },
      "source": [
        "display(df.sample(2))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>clean_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10062</th>\n",
              "      <td>Mere bhai.. afzal guru named this guy as someo...</td>\n",
              "      <td>0</td>\n",
              "      <td>mere bhai afzal guru name guy someone facilita...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8510</th>\n",
              "      <td>UP becomes the first state to begin process fo...</td>\n",
              "      <td>0</td>\n",
              "      <td>becomes first state begin process implementati...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text  ...                                         clean_text\n",
              "10062  Mere bhai.. afzal guru named this guy as someo...  ...  mere bhai afzal guru name guy someone facilita...\n",
              "8510   UP becomes the first state to begin process fo...  ...  becomes first state begin process implementati...\n",
              "\n",
              "[2 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUTDMAjxIEy_"
      },
      "source": [
        "# df.to_csv('tweets_processed.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjdsQ4-oeY3m"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wn6npGixedaN"
      },
      "source": [
        "disaster = list(df[df['target'] == 1]['clean_text'])\n",
        "non_disaster = list(df[df['target'] == 0]['clean_text'])\n",
        "\n",
        "# Creating pairs of data for siamese training => label 1 if pairs from same class otherwise 0\n",
        "df2 = pd.DataFrame(columns=['text1', 'text2', 'label'])\n",
        "\n",
        "for data in disaster:\n",
        "  data1 = data\n",
        "  data2 = random.choice(disaster)\n",
        "  data3 = random.choice(non_disaster)\n",
        "\n",
        "  df2.loc[len(df2)] = [data1, data2, 1]\n",
        "  df2.loc[len(df2)] = [data1, data3, 0]\n",
        "\n",
        "\n",
        "for data in non_disaster:\n",
        "  data1 = data\n",
        "  data2 = random.choice(non_disaster)\n",
        "  data3 = random.choice(disaster)\n",
        "  \n",
        "  df2.loc[len(df2)] = [data1, data2, 1]\n",
        "  df2.loc[len(df2)] = [data1, data3, 0]\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvVkio8nm6DK",
        "outputId": "88836551-56f6-4344-ee3b-de83cb313ada"
      },
      "source": [
        "df2.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22740, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "vMaxZO6Tec61",
        "outputId": "94329147-97f5-47ef-f2dd-45fef5d24957"
      },
      "source": [
        "display(df2.sample(5))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text1</th>\n",
              "      <th>text2</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>587</th>\n",
              "      <td>rag fire bound brook new jersey sunday destroy...</td>\n",
              "      <td>gyaabij im longer internet friend goodnight</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13877</th>\n",
              "      <td>dev flood advisory 113 944am est henderson bun...</td>\n",
              "      <td>schumer renews call suicide bomb detector penn...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>474</th>\n",
              "      <td>human body part find bag outside house north d...</td>\n",
              "      <td>iran say people arrest role ukrainian plane crash</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14542</th>\n",
              "      <td>retweet 5 second receive goodnews</td>\n",
              "      <td>yeah video make morning yey wow defs need watch …</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>847</th>\n",
              "      <td>banten province building collapse overflow riv...</td>\n",
              "      <td>final draft hip sound really sassy one sound r...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   text1  ... label\n",
              "587    rag fire bound brook new jersey sunday destroy...  ...     0\n",
              "13877  dev flood advisory 113 944am est henderson bun...  ...     0\n",
              "474    human body part find bag outside house north d...  ...     1\n",
              "14542                  retweet 5 second receive goodnews  ...     1\n",
              "847    banten province building collapse overflow riv...  ...     0\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcxiqrHCfEJu"
      },
      "source": [
        "## Data Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plFj4OyEfEJu",
        "outputId": "a88e78f7-ee1a-4a64-c3ff-011990f89824"
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(df2[['text1', 'text2']], df2['label'], test_size=0.2, random_state=0)\n",
        "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(18192, 2) (4548, 2) (18192,) (4548,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sj77LIvfEJu"
      },
      "source": [
        "X_train['text'] = X_train[['text1', 'text2']].apply(lambda x: str(x[0])+\" \"+str(x[1]), axis=1)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D8iSVUxxbos"
      },
      "source": [
        "## Glove Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rngd1uXGfEJu"
      },
      "source": [
        "t = Tokenizer()\n",
        "t.fit_on_texts(X_train['text'].values)\n",
        "\n",
        "X_train['text1'] = X_train['text1'].astype(str)\n",
        "X_train['text2'] = X_train['text2'].astype(str)\n",
        "X_val['text1'] = X_val['text1'].astype(str)\n",
        "X_val['text2'] = X_val['text2'].astype(str)\n",
        "\n",
        "train_q1_seq = t.texts_to_sequences(X_train['text1'].values)\n",
        "train_q2_seq = t.texts_to_sequences(X_train['text2'].values)\n",
        "val_q1_seq = t.texts_to_sequences(X_val['text1'].values)\n",
        "val_q2_seq = t.texts_to_sequences(X_val['text2'].values)\n",
        "\n",
        "max_len = 200\n",
        "train_q1_seq = pad_sequences(train_q1_seq, maxlen=max_len, padding='post')\n",
        "train_q2_seq = pad_sequences(train_q2_seq, maxlen=max_len, padding='post')\n",
        "val_q1_seq = pad_sequences(val_q1_seq, maxlen=max_len, padding='post')\n",
        "val_q2_seq = pad_sequences(val_q2_seq, maxlen=max_len, padding='post')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPz4Swc2fEJv",
        "outputId": "8714c2e9-2a07-4664-df70-778e60b3feac"
      },
      "source": [
        "#https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
        "embeddings_index = {}\n",
        "f = open('drive/My Drive/Glove/glove.6B.300d.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTdyVvzrfEJv",
        "outputId": "7fda7b60-c6ce-41f8-efc6-9ff2e95f5984"
      },
      "source": [
        "not_present_list = []\n",
        "vocab_size = len(t.word_index) + 1\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))\n",
        "embedding_matrix = np.zeros((vocab_size, len(embeddings_index['no'])))\n",
        "for word, i in t.word_index.items():\n",
        "    if word in embeddings_index.keys():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "    else:\n",
        "        not_present_list.append(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    else:\n",
        "        embedding_matrix[i] = np.zeros(300)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5q9WJ8GhfEJv",
        "outputId": "45b82d3a-9f9c-4dcb-d854-80f7cb531906"
      },
      "source": [
        "print(embedding_matrix.shape)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(21191, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIuQ9I6PxT-6"
      },
      "source": [
        "## Siamese Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAhYNYXbfEJw"
      },
      "source": [
        "def euclidean_distance(vectors):\n",
        "    # unpack the vectors into separate lists\n",
        "    (featsA, featsB) = vectors\n",
        "    # compute the sum of squared distances between the vectors\n",
        "    sumSquared = K.sum(K.square(featsA - featsB), axis=1, keepdims=True)\n",
        "    # return the euclidean distance between the vectors\n",
        "    return K.sqrt(K.maximum(sumSquared, K.epsilon()))\n",
        "\n",
        "def contrastive_loss(y, preds, margin=1):\n",
        "    # explicitly cast the true class label data type to the predicted\n",
        "    # class label data type (otherwise we run the risk of having two\n",
        "    # separate data types, causing TensorFlow to error out)\n",
        "    y = tf.cast(y, preds.dtype)\n",
        "    # calculate the contrastive loss between the true labels and\n",
        "    # the predicted labels\n",
        "    squaredPreds = K.square(preds)\n",
        "    squaredMargin = K.square(K.maximum(margin - preds, 0))\n",
        "    loss = K.mean(y * squaredPreds + (1 - y) * squaredMargin)\n",
        "    # return the computed contrastive loss to the calling function\n",
        "    return loss"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDW2JXPifEJw"
      },
      "source": [
        "input_1 = Input(shape=(train_q1_seq.shape[1],))\n",
        "input_2 = Input(shape=(train_q2_seq.shape[1],))\n",
        "\n",
        "common_embed = Embedding(name=\"synopsis_embedd\",input_dim =len(t.word_index)+1, \n",
        "                       output_dim=len(embeddings_index['no']),weights=[embedding_matrix], \n",
        "                       input_length=train_q1_seq.shape[1],trainable=False) \n",
        "lstm_1 = common_embed(input_1)\n",
        "lstm_2 = common_embed(input_2)\n",
        "\n",
        "\n",
        "common_lstm = LSTM(64,return_sequences=True, activation=\"relu\")\n",
        "\n",
        "vector_1 = common_lstm(lstm_1)\n",
        "vector_1 = Flatten()(vector_1)\n",
        "\n",
        "vector_2 = common_lstm(lstm_2)\n",
        "vector_2 = Flatten()(vector_2)\n",
        "\n",
        "distance = Lambda(euclidean_distance)([vector_1, vector_2])\n",
        "\n",
        "model = Model([input_1, input_2], distance)\n",
        "\n",
        "model.compile(loss=contrastive_loss, optimizer=Adam(0.00001))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTuHtjS2fEJw",
        "outputId": "637dd2c9-f8f6-4b76-8659-9dde5e2d29f4"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 200)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 200)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "synopsis_embedd (Embedding)     (None, 200, 300)     6357300     input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 200, 64)      93440       synopsis_embedd[0][0]            \n",
            "                                                                 synopsis_embedd[1][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 12800)        0           lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 12800)        0           lstm[1][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 1)            0           flatten[0][0]                    \n",
            "                                                                 flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 6,450,740\n",
            "Trainable params: 93,440\n",
            "Non-trainable params: 6,357,300\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKfZKgGQpTVk"
      },
      "source": [
        "y_train = np.asarray(y_train).astype('float32')\n",
        "y_val = np.asarray(y_val).astype('float32')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5pi0g8kfEJw",
        "outputId": "149a9dfd-b6c7-42ca-a678-84606481a3ac"
      },
      "source": [
        "model.fit([train_q1_seq,train_q2_seq],y_train.reshape(-1,1), epochs = 5, \n",
        "          batch_size=64,validation_data=([val_q1_seq, val_q2_seq],y_val.reshape(-1,1)))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "285/285 [==============================] - 116s 400ms/step - loss: 6.3506 - val_loss: 4.8329\n",
            "Epoch 2/5\n",
            "285/285 [==============================] - 113s 395ms/step - loss: 4.6287 - val_loss: 3.7624\n",
            "Epoch 3/5\n",
            "285/285 [==============================] - 113s 395ms/step - loss: 3.7560 - val_loss: 3.0697\n",
            "Epoch 4/5\n",
            "285/285 [==============================] - 113s 396ms/step - loss: 3.0429 - val_loss: 2.5800\n",
            "Epoch 5/5\n",
            "285/285 [==============================] - 113s 396ms/step - loss: 2.6172 - val_loss: 2.2104\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1ecb8fc110>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtbfF-TAgg6y",
        "outputId": "82ea50ca-c793-4df7-9072-f9bf4a0afc2a"
      },
      "source": [
        "# Save model for further use\n",
        "# serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"siamesemodel-contrastive-loss.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "#serialize weights to HDF5\n",
        "model.save_weights(\"siamesemodel-contrastive-loss.h5\")\n",
        "print(\"Saved model to disk\")\n",
        "\n",
        "# load json and create model\n",
        "# json_file = open('siamesemodel-contrastive-loss.json', 'r')\n",
        "# loaded_model_json = json_file.read()\n",
        "# json_file.close()\n",
        "# loaded_model = model_from_json(loaded_model_json)\n",
        "# # load weights into new model\n",
        "# loaded_model.load_weights(\"siamesemodel-contrastive-loss.h5\")\n",
        "# print(\"Loaded model from disk\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwpkqVdzz_Rt"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "L6D4KUQTg-hh",
        "outputId": "e4117b9f-c5c2-4b16-fba8-07855cb05ada"
      },
      "source": [
        "non_disaster[40]"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2010 inception social network easy black swan toy story 3 kid right aftershock'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knG_To6i0CMq",
        "outputId": "8329259a-3ca2-4ddc-cf1e-27b19a6a97bc"
      },
      "source": [
        "prediction_data = \"2010 inception social network easy black swan toy story 3 kid right aftershock\"\n",
        "prediction_vector = t.texts_to_sequences([prediction_data])\n",
        "prediction_vector = pad_sequences(prediction_vector,maxlen=200)\n",
        "\n",
        "assistant_data = disaster[43]\n",
        "assistant_vector = t.texts_to_sequences([assistant_data])\n",
        "assistant_vector = pad_sequences(assistant_vector,maxlen=200)\n",
        "\n",
        "model.predict([prediction_vector, assistant_vector])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.9138907]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Jo-NwC94mw4",
        "outputId": "196789cf-7bab-441b-aa4a-ac9ef44f775b"
      },
      "source": [
        "prediction_data = \"2010 inception social network easy black swan toy story 3 kid right aftershock\"\n",
        "prediction_vector = t.texts_to_sequences([prediction_data])\n",
        "prediction_vector = pad_sequences(prediction_vector,maxlen=200)\n",
        "\n",
        "assistant_data = non_disaster[12]\n",
        "assistant_vector = t.texts_to_sequences([assistant_data])\n",
        "assistant_vector = pad_sequences(assistant_vector,maxlen=200)\n",
        "\n",
        "model.predict([prediction_vector, assistant_vector])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.7457767]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aF9S_pjs5hFa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}